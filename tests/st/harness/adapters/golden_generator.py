# Copyright (c) PyPTO Contributors.
# This program is free software, you can redistribute it and/or modify it under the terms and conditions of
# CANN Open Software License Agreement Version 2.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# -----------------------------------------------------------------------------------------------------------

"""
Golden script generator for PTO testing framework.

Generates golden.py files compatible with simpler's CodeRunner.
"""

import inspect
import textwrap
from pathlib import Path
from typing import TYPE_CHECKING

import torch

if TYPE_CHECKING:
    from harness.core.harness import PTOTestCase, TensorSpec


class GoldenGenerator:
    """Generates golden.py for simpler CodeRunner.

    The generated golden.py contains:
    - generate_inputs(params): Creates input/output torch tensors
    - compute_golden(tensors, params): Computes expected outputs in-place
    - TENSOR_ORDER: List of tensor names in order
    - __outputs__: List of output tensor names
    - RTOL, ATOL: Comparison tolerances

    Example output:
        import torch

        __outputs__ = ["c"]
        TENSOR_ORDER = ["a", "b", "c"]
        RTOL = 1e-5
        ATOL = 1e-5

        def generate_inputs(params):
            return {
                "a": torch.full((128, 128), 2.0, dtype=torch.float32),
                "b": torch.full((128, 128), 3.0, dtype=torch.float32),
                "c": torch.zeros((128, 128), dtype=torch.float32),
            }

        def compute_golden(tensors, params):
            tensors["c"][:] = tensors["a"] + tensors["b"]
    """

    def generate(self, test_case: "PTOTestCase") -> str:
        """Generate golden.py content from a test case.

        Args:
            test_case: The PTOTestCase to generate golden for.

        Returns:
            Python source code for golden.py.
        """
        tensor_specs = test_case.tensor_specs
        config = test_case.config

        # Get output names
        output_names = [t.name for t in tensor_specs if t.is_output]
        tensor_order = [t.name for t in tensor_specs]

        lines = [
            '"""',
            f"Auto-generated golden script for {test_case.get_name()}.",
            "",
            "Generated by harness.adapters.GoldenGenerator",
            '"""',
            "",
            "import torch",
            "",
            f"__outputs__ = {output_names!r}",
            f"TENSOR_ORDER = {tensor_order!r}",
            f"RTOL = {config.rtol}",
            f"ATOL = {config.atol}",
            "",
        ]

        # Generate generate_inputs function
        lines.append("def generate_inputs(params):")
        lines.append('    """Generate input and output tensors."""')
        lines.append("    return {")

        for spec in tensor_specs:
            init_code = self._generate_init_code(spec)
            lines.append(f'        "{spec.name}": {init_code},')

        lines.append("    }")
        lines.append("")

        # Generate compute_golden function by extracting compute_expected source
        lines.append("")
        compute_golden_code = self._generate_compute_golden(test_case)
        lines.extend(compute_golden_code.split("\n"))
        lines.append("")

        return "\n".join(lines)

    def _generate_init_code(self, spec: "TensorSpec") -> str:
        """Generate torch initialization code for a tensor spec."""
        dtype_str = self._dtype_to_torch_str(spec.dtype)
        shape_str = repr(tuple(spec.shape))

        if spec.is_output or spec.init_value is None:
            return f"torch.zeros({shape_str}, dtype={dtype_str})"
        elif isinstance(spec.init_value, (int, float)):
            return f"torch.full({shape_str}, {spec.init_value!r}, dtype={dtype_str})"
        elif callable(spec.init_value):
            return self._generate_callable_init(spec, shape_str, dtype_str)
        elif isinstance(spec.init_value, torch.Tensor):
            return self._generate_tensor_init(spec, shape_str, dtype_str)
        else:
            # Assume it's array-like
            return f"torch.zeros({shape_str}, dtype={dtype_str})  # TODO: handle tensor init"

    def _generate_callable_init(self, spec: "TensorSpec", shape_str: str, dtype_str: str) -> str:
        """Generate initialization code for callable init_value."""
        init_fn = spec.init_value
        # Support a limited set of known torch initializers that can be
        # faithfully represented in generated source code.
        if init_fn is torch.randn:
            return f"torch.randn({shape_str}, dtype={dtype_str})"
        if init_fn is torch.zeros:
            return f"torch.zeros({shape_str}, dtype={dtype_str})"
        if init_fn is torch.ones:
            return f"torch.ones({shape_str}, dtype={dtype_str})"
        # Other callables cannot be safely or deterministically reproduced
        # in the generated golden.py. Force the user to provide a supported
        # initializer instead of silently changing semantics.
        raise ValueError(
            f"Callable init_value={init_fn!r} for tensor {spec.name!r} "
            "is not supported by GoldenGenerator. "
            "Use a scalar, a tensor, or a supported torch initializer "
            "(torch.randn, torch.zeros, torch.ones)."
        )

    def _generate_tensor_init(self, spec: "TensorSpec", shape_str: str, dtype_str: str) -> str:
        """Generate initialization code for torch.Tensor init_value."""
        arr = spec.init_value

        # Check if all elements are the same (constant array)
        if arr.numel() > 0 and torch.all(arr == arr.flatten()[0]):
            constant_val = arr.flatten()[0].item()
            if constant_val == 0:
                return f"torch.zeros({shape_str}, dtype={dtype_str})"
            elif constant_val == 1:
                return f"torch.ones({shape_str}, dtype={dtype_str})"
            else:
                return f"torch.full({shape_str}, {constant_val!r}, dtype={dtype_str})"

        # Check for identity matrix
        if len(arr.shape) == 2 and arr.shape[0] == arr.shape[1]:
            if torch.allclose(arr, torch.eye(arr.shape[0])):
                return f"torch.eye({arr.shape[0]}, dtype={dtype_str})"

        # For small tensors (< 100 elements), serialize directly
        if arr.numel() <= 100:
            # Convert tensor to Python list for serialization
            arr_list = arr.tolist()
            return f"torch.tensor({arr_list!r}, dtype={dtype_str})"

        # For larger tensors, suggest using callable or saving to file
        return (
            f"torch.zeros({shape_str}, dtype={dtype_str})  "
            f"# Warning: Tensor too large ({arr.numel()} elements) to serialize. "
            f"Consider using callable init_value or loading from file."
        )

    def _dtype_to_torch_str(self, dtype) -> str:
        """Convert DataType enum to torch dtype string."""
        from harness.core.harness import DataType  # noqa: PLC0415

        mapping = {
            DataType.FP32: "torch.float32",
            DataType.FP16: "torch.float16",
            DataType.INT32: "torch.int32",
            DataType.INT64: "torch.int64",
            DataType.BOOL: "torch.bool",
        }
        return mapping.get(dtype, "torch.float32")

    def _generate_compute_golden(self, test_case: "PTOTestCase") -> str:
        """Generate compute_golden function from compute_expected source.

        Extracts the compute_expected method source and renames it to compute_golden.
        Since both methods have the same signature now (tensors, params), we just
        need to rename the function and remove one level of indentation.

        Args:
            test_case: The PTOTestCase containing compute_expected method.

        Returns:
            Source code for compute_golden function.
        """
        try:
            # Get the source code of compute_expected
            source = inspect.getsource(test_case.compute_expected)

            # Replace method name and remove 'self' parameter
            lines = source.split("\n")
            result_lines = []

            for line in lines:
                if "def compute_expected" in line:
                    # Replace method signature with standalone function
                    # Original: def compute_expected(self, tensors: Dict[str, torch.Tensor],
                    #           params: Optional[Dict[str, Any]] = None) -> None:
                    # New: def compute_golden(tensors, params):
                    # We'll do a simple replacement to handle various formatting
                    new_line = line.replace("def compute_expected(self, ", "def compute_golden(")
                    # Remove type hints and return type annotation for simplicity
                    if ":" in new_line and "->" in new_line:
                        # Remove return type annotation
                        new_line = new_line.split("->")[0].rstrip() + ":"
                    # Remove one level of indentation (convert method to function)
                    if new_line.startswith("    "):
                        new_line = new_line[4:]
                    result_lines.append(new_line)
                # Remove one level of indentation for all other lines
                elif line.startswith("    "):
                    result_lines.append(line[4:])
                else:
                    result_lines.append(line)

            return "\n".join(result_lines)

        except (TypeError, OSError):
            # Fallback: generate a placeholder
            output_specs = test_case.get_output_tensors()
            lines = [
                "def compute_golden(tensors, params):",
                '    """Compute expected outputs - PLACEHOLDER."""',
                "    # TODO: Could not extract compute_expected source.",
                "    # Please implement the expected computation here.",
            ]
            for spec in output_specs:
                lines.append(f'    # tensors["{spec.name}"][:] = ...')
            lines.append("")
            lines.append('    raise NotImplementedError("compute_expected source extraction failed")')
            return "\n".join(lines)

    def write(self, test_case: "PTOTestCase", output_path: Path) -> Path:
        """Generate and write golden.py.

        Args:
            test_case: The PTOTestCase to generate golden for.
            output_path: Path to write golden.py.

        Returns:
            Path to the written golden.py file.
        """
        content = self.generate(test_case)
        output_path = Path(output_path)
        output_path.write_text(content)
        return output_path


class InlineGoldenGenerator(GoldenGenerator):
    """Golden generator that inlines the compute_expected logic.

    This generator creates a golden.py that directly calls back
    to the test case's compute_expected method. This is useful
    when the computation is complex and cannot be easily inlined.
    """

    def generate_with_callback(
        self,
        test_case: "PTOTestCase",
        compute_code: str,
    ) -> str:
        """Generate golden.py with explicit compute code.

        Args:
            test_case: The PTOTestCase.
            compute_code: Python code string for computing expected outputs.
                         Should modify tensors dict in-place.

        Returns:
            Python source code for golden.py.
        """
        tensor_specs = test_case.tensor_specs
        config = test_case.config

        output_names = [t.name for t in tensor_specs if t.is_output]
        tensor_order = [t.name for t in tensor_specs]

        lines = [
            '"""',
            f"Golden script for {test_case.get_name()}.",
            '"""',
            "",
            "import torch",
            "",
            f"__outputs__ = {output_names!r}",
            f"TENSOR_ORDER = {tensor_order!r}",
            f"RTOL = {config.rtol}",
            f"ATOL = {config.atol}",
            "",
        ]

        # Generate generate_inputs
        lines.append("def generate_inputs(params):")
        lines.append("    return {")
        for spec in tensor_specs:
            init_code = self._generate_init_code(spec)
            lines.append(f'        "{spec.name}": {init_code},')
        lines.append("    }")
        lines.append("")

        # Generate compute_golden with provided code
        lines.append("")
        lines.append("def compute_golden(tensors, params):")
        # Indent the provided compute code
        indented_code = textwrap.indent(compute_code.strip(), "    ")
        lines.append(indented_code)
        lines.append("")

        return "\n".join(lines)
